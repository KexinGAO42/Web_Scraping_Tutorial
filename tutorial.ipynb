{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# [Problem Description](https://mail-attachment.googleusercontent.com/attachment/u/0/?ui=2&ik=8ef9393342&attid=0.1&permmsgid=msg-f:1778848600873199608&th=18afbd60b43f67f8&view=att&disp=inline&saddbat=ANGjdJ_gZ385uRPgINR8vBAyS3cKq3hn29lcGA517jDFeLyZp2DJBEDpf7ToaI40-lsda-0rIqzFvQEMIzEq_btFwGRbeH3q_hUOEl3HCcNSYg_GeD5muJARpndtUMQTn_bs3k6g5yvYVbsIewDpXnIn6lvtS2nYZN3UuMAWAkZXLXrJiLz1tAoykpN9wARIqRIefxrty-wMfGhCwzDC_a295wY3KkdJOlrm1P1KXn4UEqoZIMqFr0O1Lx-5OhJmNI0bNmhwomjpb_0zsFxHUrt_qNGMdXVjAQp1j3EOChWy3uHrbavT_z73dUiga2JJ6kHdagRuPWfYVFCEhOE2NyLr7sTs5KTnRfSOUeb2vFu_ooVWM1saJFqgJmhm-pP8JDTzEIYC2jRy9EHkfM85jhEw7Wu_zngyBilbircPWsCl74HlqVPmTxf_GJ3betFGHizn4-rQ1YAjhQJB8FmchgFRNNIXCm3ilc678SOVu8nW_ATC6w61iePxMeJwiyZTbpQe_HR2Y9vHymQqqtzboF9Ux8UqEzm1NGsLDVAWPx4ELrHqMH_6g8sRpE6bqkdcRaopIG58QGxmfJJo0I4wOZCQU3nm3FQErm4GvqdsnT6Vr7eN-vr8A_EcnqoYMy5DS78x6NpX0z61mqP2A83cuuDm3ufZRJha7x2Zd4hB1Ynoc4r5IVENT6pPODyJRoCvmpd2zkupVNo94IufNXMWKbiXsU27FRvVqVbzLRFrxooyY9csRYBsj_UlxEQFo15oaEHOX03U_qh1rSJuey0U13hd0VdtuASyQ5JjtJW94UyWqWyuIoPkVbOEKaFGY7kFqJsDnzPQjeqex6YkPtHIj-6nJ_7xq3TxpXdRQ3YbqpIh9YAV3yLrTk3Ow2YyXpOG2NjUq_xlgeVF-KwXDq0_L2VOZLLOnjYzQawpQlbUnrr7bkKmFG1XnRj8JIRO9iE84VsLM85-eFlNB08pi7akcDPVOHM2VuYnhLWVJp_yg1QvBPlUB67EdSzHgznOgQYhttps://mail-attachment.googleusercontent.com/attachment/u/0/?ui=2&ik=8ef9393342&attid=0.1&permmsgid=msg-f:1778848600873199608&th=18afbd60b43f67f8&view=att&disp=inline&saddbat=ANGjdJ_gZ385uRPgINR8vBAyS3cKq3hn29lcGA517jDFeLyZp2DJBEDpf7ToaI40-lsda-0rIqzFvQEMIzEq_btFwGRbeH3q_hUOEl3HCcNSYg_GeD5muJARpndtUMQTn_bs3k6g5yvYVbsIewDpXnIn6lvtS2nYZN3UuMAWAkZXLXrJiLz1tAoykpN9wARIqRIefxrty-wMfGhCwzDC_a295wY3KkdJOlrm1P1KXn4UEqoZIMqFr0O1Lx-5OhJmNI0bNmhwomjpb_0zsFxHUrt_qNGMdXVjAQp1j3EOChWy3uHrbavT_z73dUiga2JJ6kHdagRuPWfYVFCEhOE2NyLr7sTs5KTnRfSOUeb2vFu_ooVWM1saJFqgJmhm-pP8JDTzEIYC2jRy9EHkfM85jhEw7Wu_zngyBilbircPWsCl74HlqVPmTxf_GJ3betFGHizn4-rQ1YAjhQJB8FmchgFRNNIXCm3ilc678SOVu8nW_ATC6w61iePxMeJwiyZTbpQe_HR2Y9vHymQqqtzboF9Ux8UqEzm1NGsLDVAWPx4ELrHqMH_6g8sRpE6bqkdcRaopIG58QGxmfJJo0I4wOZCQU3nm3FQErm4GvqdsnT6Vr7eN-vr8A_EcnqoYMy5DS78x6NpX0z61mqP2A83cuuDm3ufZRJha7x2Zd4hB1Ynoc4r5IVENT6pPODyJRoCvmpd2zkupVNo94IufNXMWKbiXsU27FRvVqVbzLRFrxooyY9csRYBsj_UlxEQFo15oaEHOX03U_qh1rSJuey0U13hd0VdtuASyQ5JjtJW94UyWqWyuIoPkVbOEKaFGY7kFqJsDnzPQjeqex6YkPtHIj-6nJ_7xq3TxpXdRQ3YbqpIh9YAV3yLrTk3Ow2YyXpOG2NjUq_xlgeVF-KwXDq0_L2VOZLLOnjYzQawpQlbUnrr7bkKmFG1XnRj8JIRO9iE84VsLM85-eFlNB08pi7akcDPVOHM2VuYnhLWVJp_yg1QvBPlUB67EdSzHgznOgQY)\n"
      ],
      "metadata": {
        "id": "ZAZ-odWnPFjG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pre-requisites\n",
        "\n",
        "**Please run the cell below to install the required package: nltk, requests and bs4.** \\\\\n",
        "**If you've already got these pakages in your environment, you can skip this step.**"
      ],
      "metadata": {
        "id": "KwkWIG5z9epq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "!pip install requests\n",
        "!pip install bs4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4jRHZYR9Dg1",
        "outputId": "c36eddca-1e43-48e2-aca2-caddad637bcb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2023.7.22)\n",
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.10/dist-packages (0.0.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from bs4) (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->bs4) (2.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1: Selecting Website and Scraping Raw Data\n",
        "\n",
        "In this part, we choose two websites, from each of which we can scrape around 60 recipes. \\\\\n",
        "The dessert recipes website: https://www.simplyrecipes.com/dessert-recipes-5091513 \\\\\n",
        "The breakfast recipes website: https://www.simplyrecipes.com/breakfast-recipes-5091541 \\\\\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "After selecting the websites, we need to carefully investigate the source code of the webpage, and find out how can we get the information that we need: \\\\\n",
        "\n",
        "\n",
        "1.   The link of the recipes\n",
        "2.   The name of the recipes\n",
        "3.   The ingredients of the recipes\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "By investigating the struture of the website source code, we can find out that:\n",
        "\n",
        "\n",
        "1.   The 60 recipes on each page are orgnized by \"id\" range from \"2-0\" (equals to \"2-0-0\") to \"2-0-59\"\n",
        "2.   Through the id we can get the link and the name of each recipe\n",
        "3.   Through the link of each recipe, we can get to the detail page of the recipe, and find the ingredient lists\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Now we've known where to get the data we want, let's do the coding of this part.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Etp7BSH_Y54M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: generate the id list of the recipe\n",
        "# This id list is avaible for both two websites we selected\n",
        "\n",
        "recipe_id_lists = [\"mntl-card-list-items_2-0\"]\n",
        "id_root = \"mntl-card-list-items_2-0-\"\n",
        "for i in range(1, 60):\n",
        "  recipe_id_lists.append(id_root + str(i))"
      ],
      "metadata": {
        "id": "LFD9_LCTbcYv"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: define the functions to scrape the link, name and ingredients from each recipe\n",
        "# Here we will utilize two useful python packages for web scraping:\n",
        "# requests: https://pypi.org/project/requests/\n",
        "# BeautifulSoup: https://pypi.org/project/beautifulsoup4/\n",
        "\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def prepare_soup(link):\n",
        "  page = requests.get(link)\n",
        "  soup = BeautifulSoup(page.content, \"html.parser\")\n",
        "  return soup\n",
        "\n",
        "def get_link_from_id(id, soup):\n",
        "  recipe_link = soup.find(\"a\", id=id).get('href')\n",
        "  return recipe_link\n",
        "\n",
        "def get_name_from_id(id, soup):\n",
        "  recipe_name = soup.find(id=id).find(\"span\", class_=\"card__title-text\").text\n",
        "  return recipe_name\n",
        "\n",
        "def get_ingre_from_link(recipe_link):\n",
        "  page = requests.get(recipe_link)\n",
        "  soup = BeautifulSoup(page.content, \"html.parser\")\n",
        "  ingre_content = soup.find(\"div\", class_=\"comp structured-ingredients\").find_all(\"ul\", class_=\"structured-ingredients__list text-passage\")\n",
        "  ingredients = []\n",
        "  for i in ingre_content:\n",
        "    ingres = i.find_all(\"li\", class_=\"structured-ingredients__list-item\")\n",
        "    for ingre in ingres:\n",
        "      ingredients.append(ingre.text.rstrip().lstrip())\n",
        "  return ingredients\n"
      ],
      "metadata": {
        "id": "JHitE8vHPfUG"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: get the data from the recipes on the two websites, and organize the data into 3 lists (name, link, ingredients)\n",
        "# This step may take 1-2 minutes\n",
        "\n",
        "websites = ['https://www.simplyrecipes.com/dessert-recipes-5091513', 'https://www.simplyrecipes.com/breakfast-recipes-5091541']\n",
        "recipe_name, recipe_link, recipe_ingredients = [], [], []\n",
        "\n",
        "for website in websites:\n",
        "\n",
        "  soup = prepare_soup(website)\n",
        "\n",
        "  for id in recipe_id_lists:\n",
        "    name = get_name_from_id(id, soup)\n",
        "    link = get_link_from_id(id, soup)\n",
        "\n",
        "    # Since not all links contains a formatted ingredient list, we will skip the links that cannot fetch the ingredient list\n",
        "\n",
        "    try:\n",
        "      ingre = get_ingre_from_link(link)\n",
        "      recipe_name.append(name)\n",
        "      recipe_link.append(link)\n",
        "      recipe_ingredients.append(ingre)\n",
        "\n",
        "    except AttributeError:\n",
        "      pass\n",
        "\n",
        "# save the total number of recipes for further calculation\n",
        "num_of_recipe = len(recipe_name)"
      ],
      "metadata": {
        "id": "ZXrqOivDo4RA"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: now we write the raw data to a csv file, and return rawData.csv\n",
        "# After running this cell, you will have the rawData.csv file in your file folder\n",
        "\n",
        "import csv\n",
        "\n",
        "with open('rawData.csv', \"w\") as csvfile:\n",
        "  writer = csv.DictWriter(csvfile, fieldnames = [\"url\", \"name\", \"ingredient\"])\n",
        "  writer.writeheader()\n",
        "  for i in range(len(recipe_ingredients)):\n",
        "    for ingre in recipe_ingredients[i]:\n",
        "      writer.writerow({\"url\": recipe_link[i], \"name\": recipe_name[i], \"ingredient\":ingre})\n"
      ],
      "metadata": {
        "id": "sfsL1qet8XGL"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2: Cleaning the data\n",
        "In this part, we need to clean the text of \"ingredients\" for furuther calculation. \\\\\n",
        "Our goal is to keep only the name of the ingredients to see the frequency that the ingredients are used, thus we need to:\n",
        "\n",
        "1.   remove the number and the measurement nouns in the text (e.g. \"1/2 teaspoon kosher salt\" -> \"kosher salt\")\n",
        "2.   convert the plural form to the singular form (e.g. \"eggs\" -> \"egg\")\n",
        "3.   make the results more detailed, we choose to keep words that are nouns\n",
        "4.   deal with other edge words\n",
        "\n",
        "\n",
        "In order to finish these tasks, we will implement several NLP techniques including POS tagging, word lemmatizing, wordnet and other methods.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cHAGrBVAf9DF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: run this cell to import and download all the resources we need\n",
        "\n",
        "import nltk\n",
        "from nltk import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXo51hpN_gWR",
        "outputId": "e3ec4ba2-0eb1-4889-b7de-f7d5d89c1bda"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: define the functions for cleaning the data\n",
        "\n",
        "# Define a function to check whether the word is noun, adjective or past tense verb\n",
        "# If true, later we will keep the word, else we will remove the word\n",
        "\n",
        "def is_noun(tag):\n",
        "  if tag.startswith('NN'):\n",
        "    return True\n",
        "  else:\n",
        "    return False\n",
        "\n",
        "# Define a function to convert plural form into singular form\n",
        "\n",
        "def plural_to_singular(word):\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  singular_form = lemmatizer.lemmatize(word, pos=wordnet.NOUN)\n",
        "  return singular_form\n",
        "\n",
        "# Dealing with edge cases\n",
        "\n",
        "# Define a function to check whther the words contain \"-\" (e.g. 12-ounce, 0.5-teaspoon)\n",
        "# If false, later we will keep the word, else we will remove the word\n",
        "\n",
        "def has_dash(word):\n",
        "  if '-' in word:\n",
        "    return True\n",
        "  else:\n",
        "    return False\n",
        "\n",
        "# Maintain a black list for removing words that are nouns, adjectives or past tense verbs but we don't want (most of the words are measurement words)\n",
        "black_list = ['cup', 'teaspoon', 'tablespoon', 'ounce', 'gram', 'pound', 'liter', 'pint', 'jar', 'room', 'temperature', 'pan', 'F', 'such']\n",
        "\n"
      ],
      "metadata": {
        "id": "Iv7Bj8RqE18k"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: use the methods we defined above to write a function for cleaning the data\n",
        "\n",
        "def clean_ingredients(original_ingredients):\n",
        "\n",
        "  cleaned_ingredients = []\n",
        "\n",
        "  for ingredients in original_ingredients: # each list of the lists of ingredients\n",
        "\n",
        "    cleaned_list = []\n",
        "\n",
        "    for ingredient in ingredients: # each ingredient phrase of the list of ingredients\n",
        "\n",
        "      words = word_tokenize(ingredient) # list of words in the ingredient phrase\n",
        "      words = [plural_to_singular(word) for word in words] # convert plural form to singular\n",
        "      tagged_words = pos_tag(words) # get POS tag of the words\n",
        "      tokens = [word.lower() for word, tag in tagged_words if is_noun(tag) and word.lower() not in black_list and not has_dash(word)] # clean the words\n",
        "      if len(tokens) != 0:\n",
        "        ingredients = ' '.join(tokens)\n",
        "      cleaned_list.append(ingredients)\n",
        "\n",
        "    cleaned_ingredients.append(cleaned_list)\n",
        "\n",
        "  return cleaned_ingredients\n"
      ],
      "metadata": {
        "id": "gRpJUZYIFPoU"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: run the cell to get a cleaned ingredients list\n",
        "\n",
        "cleaned_ingredients = clean_ingredients(recipe_ingredients)"
      ],
      "metadata": {
        "id": "DdwwRUwNIKB2"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: now we write the clean data to a csv file, and return cleanData.csv\n",
        "# After running this cell, you will have the cleanData.csv file in your file folder\n",
        "\n",
        "with open('cleanData.csv', \"w\") as csvfile:\n",
        "  writer = csv.DictWriter(csvfile, fieldnames = [\"url\", \"name\", \"ingredient\"])\n",
        "  writer.writeheader()\n",
        "  for i in range(len(cleaned_ingredients)):\n",
        "    for ingre in cleaned_ingredients[i]:\n",
        "      writer.writerow({\"url\": recipe_link[i], \"name\": recipe_name[i], \"ingredient\":ingre})\n"
      ],
      "metadata": {
        "id": "YKF2vBVmIZpu"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 3: Calculating\n",
        "\n",
        "Now we've got all cleaned ingredients appeared in our scraped recipes. We need to use the data to calculate:\n",
        "\n",
        "\n",
        "1.   count: the number of times the ingredient appears\n",
        "2.   proportion: the proportion of recipes in which the  ingredient appears (i.e., num_of_recipes_that_contain_this_ingredient / num_of_recipe)\n",
        "\n",
        "To perform the calculation, we use a hashmap to store the ingredient (as key) and with a value of a list [count, num_of_recipes_containing_this_ingredient].\n",
        "\n"
      ],
      "metadata": {
        "id": "uxGgDqxSnpA9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: perform the counting and store the results into a hashmap\n",
        "\n",
        "ingre_calculation = {}\n",
        "\n",
        "for ingre_list in cleaned_ingredients:\n",
        "  visited_ingre = set()\n",
        "  for ingre in ingre_list:\n",
        "    if ingre not in ingre_calculation.keys():\n",
        "      ingre_calculation[ingre] = [1, 0]\n",
        "    else:\n",
        "      ingre_calculation[ingre][0] += 1\n",
        "    if ingre not in visited_ingre:\n",
        "      visited_ingre.add(ingre)\n",
        "      ingre_calculation[ingre][1] += 1\n",
        "\n",
        "# sort the hashmap by the count of the ingredients\n",
        "ingre_calculation = {k: v for k, v in sorted(ingre_calculation.items(), key=lambda item: item[1][0], reverse=True)}\n"
      ],
      "metadata": {
        "id": "VAgGfOKHJJE4"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: get the top ten ingredients\n",
        "\n",
        "from itertools import islice\n",
        "\n",
        "def take(n, iterable):\n",
        "    return list(islice(iterable, n))\n",
        "\n",
        "top_ten_ingredients = take(10, ingre_calculation.items())"
      ],
      "metadata": {
        "id": "m2bT3Yw53tFV"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: now we write the top ten result to a csv file, and return results.csv\n",
        "# After running this cell, you will have the results.csv file in your file folder\n",
        "\n",
        "with open('results.csv', \"w\") as csvfile:\n",
        "  writer = csv.DictWriter(csvfile, fieldnames = [\"word\", \"count\", \"proportion\"])\n",
        "  writer.writeheader()\n",
        "  for item in top_ten_ingredients:\n",
        "    writer.writerow({\"word\": item[0], \"count\": item[1][0], \"proportion\": round(item[1][1]/num_of_recipe, 2)})"
      ],
      "metadata": {
        "id": "j_J0pM-jUUrI"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# We Finished! Hooray!\n",
        "Now we can go check our results! Thank you for walking through this project!"
      ],
      "metadata": {
        "id": "zGe2Se0v_h_c"
      }
    }
  ]
}